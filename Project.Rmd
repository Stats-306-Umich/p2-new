---
title: "DataSci 306 Final Project"
author: "Hania Timek, Shaurya Pratap Singh, Aden Tao"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
always_allow_html: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

```

*  Hania Timek positcloud has the r script files for the applications in part 6

* Shaurya Pratap Singh has the r script files for the applications in extra credit. Under the folder "application3"

## Investigating the Internet Movie Database (IMDB)

The [Internet Movie Database (IMDb)]() contains information on millions of movies and television programs. They offer several [non-commercial use datasets](https://developer.imdb.com/non-commercial-datasets/) (documentation link). For this project we will analyze a **sample** of 100,000 titles from the IMDBb. 


## Part I: Preprocessing
* [Edit your `.gitignore` file](https://docs.github.com/en/get-started/getting-started-with-git/ignoring-files) to ignore all files with the `.rda` extension. (Add and commit)
* Create a new file in the `data/` directory called "Preprocessing.Rmd". The remaining instructions in this section are to be completed in that file.
* Write a function that will load a table from the IMDb files in the `data/` directory.
  * The function should take the file name (without the ".csv.gz" portion) as an argument
  * The function should load the appropriate `.csv.gz` file.
  * Make sure that all "\\N" values (which IMDB uses to indicate missing values) are turned into proper NA values in R
  * The function should return the table.
* For each of the `.csv.gz` files, use your function to load the table, then save it into a variable (e.g. `name_basics <- preprocess("name_basics")`) and use the `write_rds` function (e.g., `write_rds(name_basics, "name_basics.rda")`.
* Run the function on all of the `*_sample.csv.gz` files to created processed `.rda` files.
* In your other files, you can load these using the `TABLE <- read_rds("data/FILENAME.rda")` function.

```{r}
name_basics <- read_rds("data/name_basics.rda")
title_basics <- read_rds("data/title_basics.rda")
title_principals <- read_rds("data/title_principals.rda")
title_ratings <- read_rds("data/title_ratings.rda")
```

## Part II: EDA of individual tables (aden)

* For each of the 4 tables, perform basic exploratory data analysis. Report the following information:
  * For each quantitative column, provide some summary statistics
  * For any character columns, decided if they are actually representing factors/categorical data with a moderate number of columns. If so report the distributions for these variables.
  
```{r}
library(dplyr)
library(tidyr)

eda_summary <- function(df, df_name) {
  cat("\n============================\n")
  cat("Summary for", df_name, "\n")
  cat("============================\n")
  
  # Quantitative Columns
  num_cols <- sapply(df, is.numeric)
  if (any(num_cols)) {
    cat("\nQuantitative Columns Summary:\n")
    print(summary(df[, num_cols]))
  } else {
    cat("\nNo Quantitative Columns.\n")
  }
  
  # Character Columns
  char_cols <- sapply(df, is.character)
  if (any(char_cols)) {
    cat("\nCharacter Columns (Checking if Categorical):\n")
    for (colname in names(df)[char_cols]) {
      # Special case: if the column has commas (like genres), split it first
      if (any(grepl(",", df[[colname]], fixed = TRUE))) {
        split_values <- unlist(strsplit(df[[colname]], ","))
        split_values <- trimws(split_values)  # remove spaces
        n_unique <- length(unique(split_values))
        
        cat("\n", colname, "(split on commas) - Unique Values:", n_unique, "\n")
        
        if (n_unique <= 50) {
          cat("Likely Categorical after splitting. Distribution:\n")
          print(table(split_values))
        } else {
          cat("Not categorical (too many unique values after splitting).\n")
        }
        
      } else {
        n_unique <- n_distinct(df[[colname]])
        cat("\n", colname, "- Unique Values:", n_unique, "\n")
        
        if (n_unique <= 30) {
          cat("Likely Categorical. Distribution:\n")
          print(table(df[[colname]]))
        } else {
          cat("Not categorical (too many unique values).\n")
        }
      }
    }
  } else {
    cat("\nNo Character Columns.\n")
  }
}


# Run EDA for all 4 tables
eda_summary(name_basics, "name_basics")
eda_summary(title_basics, "title_basics")
eda_summary(title_principals, "title_principals")
eda_summary(title_ratings, "title_ratings")
```

  * Provide a plot for each table. Across all of the plots, try to show off the most possible different ggplot features (`geoms_` functions, `stat_` functions, coordinate systems, facets, use of several variables, annotations)
  
```{r}
ggplot(name_basics, aes(x = birthYear)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  geom_density(aes(y = ..count..), color = "red") +
  labs(title = "Distribution of Birth Years", x = "Birth Year", y = "Count") +
  theme_minimal()

title_basics_long <- title_basics %>%
  tidyr::separate_rows(genres, sep = ",") %>%
  filter(!is.na(genres))

ggplot(title_basics_long, aes(x = genres)) +
  geom_bar(fill = "skyblue") +
  coord_flip() +
  labs(title = "Count of Movies by Genre", x = "Genre", y = "Count") +
  theme_bw()
```

```{r}
title_plot <- title_basics %>%
  filter(titleType %in% c("movie", "tvSeries", "short")) %>%
  mutate(runtimeMinutes = as.numeric(runtimeMinutes)) %>%
  ggplot(aes(titleType, runtimeMinutes)) +
  geom_violin(aes(fill = titleType), show.legend = FALSE) +
  geom_boxplot(width = 0.2, outlier.shape = NA) +
  stat_summary(fun = "median", geom = "point", size = 2, color = "red") +
  scale_y_log10() +
  facet_wrap(~titleType, scales = "free_x") +
  labs(title = "Runtime Distribution by Title Type",
       caption = "Log scale for better visibility") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

title_plot
```

```{r}
title_basics %>%
  filter(primaryTitle != originalTitle) %>%
  count()
title_basics %>%
  mutate(name_discrepancy = ifelse(primaryTitle != originalTitle, "Different", "Same")) %>%
  ggplot(aes(x = startYear, fill = name_discrepancy)) +
  geom_density(alpha = 0.5) +
  labs(title = "Release Year Distribution by Name Discrepancy")
```

```{r}
principals_plot <- title_principals %>%
  left_join(title_basics %>% select(tconst, startYear), by = "tconst") %>%
  filter(startYear >= 1900, startYear <= 2023) %>%
  mutate(decade = floor(startYear/10)*10) %>%
  count(decade, category) %>%
  ggplot(aes(decade, category)) +
  geom_tile(aes(fill = n), color = "white") +
  geom_text(aes(label = scales::comma(n)), color = "black", size = 3) +
  scale_fill_gradient(low = "yellow", high = "orange") +
  scale_x_continuous(breaks = seq(1900, 2020, 20)) +
  labs(title = "Film Roles Through the Decades",) +
  theme_minimal() +
  theme(panel.grid = element_blank())

principals_plot
```

```{r}
ratings_plot <- title_ratings %>%
  ggplot(aes(averageRating, numVotes)) +
  geom_hex(bins = 50) +
  geom_smooth(method = "gam", color = "red") +
  annotate("text", x = 2, y = 1e6, 
           label = paste("Correlation:", 
                         round(cor(title_ratings$averageRating, 
                                   log(title_ratings$numVotes)), 2))) +  # Explicit reference
  scale_y_log10(labels = scales::comma) +
  scale_fill_viridis_c(option = "magma") +
  labs(title = "Rating vs. Number of Votes") +
  theme_bw()

ratings_plot
```
  * How many titles are known for name that is different than the original release name?

```{r}
count <- sum(title_basics$primaryTitle != title_basics$originalTitle, na.rm = TRUE)
count
```
  * Graph the conditional distributions of release year based on the previous results.
  
```{r}
title_basics %>%
  mutate(
    name_diff = primaryTitle != originalTitle,
    startYear = as.numeric(startYear)
  ) %>%
  filter(!is.na(startYear), startYear >= 1900) %>%
  ggplot(aes(startYear, fill = name_diff)) +
  geom_density(alpha = 0.5) +
  labs(title = "conditional distributions of release year") +
  theme_minimal()
```

Comment on any trends you observe.

Titles with different names are more common in recent decades and this suggests increasing rebranding or localization efforts in modern media.

* For the ratings, use the `cut` function to break the data into three groups based on the average ratings. Are higher rated titles rated more often or less often than lower rated titles? 

```{r}
ratings_group <- title_ratings %>%
  mutate(
    rating_group = cut(
      averageRating,
      breaks = c(0, 5, 7, 10),
      labels = c("Low (0-5)", "Medium (5-7)", "High (7-10)")
    )
  ) %>%
  group_by(rating_group) %>%
  summarise(median_votes = median(numVotes, na.rm = TRUE))

ratings_group
```

* For the names table, 
  * Count the number of titles each person is known for and plot this distribution.
  
```{r}
name_counts <- name_basics %>%
  mutate(num_titles = lengths(strsplit(knownForTitles, ","))) %>%
  filter(!is.na(knownForTitles)) 

(name_counts)
```

```{r}
name_counts %>%
  ggplot(aes(num_titles)) +
  geom_bar(fill = "blue") +
  labs(
    title = "Number of 'Known For' Titles Per Person",
    x = "Number of Titles",
    y = "Count"
  )
```

  * investigate the age of cast members
      * Group the data into living and deceased cast members.
      
```{r}
name_ages <- name_basics %>%
  mutate(
    status = ifelse(is.na(deathYear), "Living", "Deceased"),
    age = ifelse(
      status == "Deceased",
      deathYear - birthYear,
      2025 - birthYear
    )
  ) %>%
  filter(age > 0, age < 120)

name_ages
```

      * For deceased cast members, provide a graph that shows the distribution of ages.
      
```{r}
name_ages %>%
  filter(status == "Deceased") %>%
  ggplot(aes(age)) +
  geom_histogram(binwidth = 5, fill = "blue") +
  labs(title = "Age Distribution")
```

      * Do the same for living cast members.
      
```{r}
name_ages %>%
  filter(status == "Living") %>%
  ggplot(aes(age)) +
  geom_histogram(binwidth = 5, fill = "blue") +
  labs(title = "Age Distribution of Living Cast Members")
```

* Find all the actors with first names "Tom", "Thomas", "Thom" or "Tomas". How many are there?

```{r}
tom_actors <- name_basics %>%
  mutate(first_name = word(primaryName, 1)) %>%
  filter(
    str_to_lower(first_name) %in% c("tom", "thomas", "thom", "tomas")
  )

nrow(tom_actors)
```


* How many titles use alliteration (i.e., all words in the title start with the same letter)?

```{r}
title_alliteration <- title_basics %>%
  mutate(
    title_upper = str_to_upper(primaryTitle),
    words = str_split(title_upper, "\\s+"),
    first_letters = map(words, ~ str_sub(.x, 1, 1)),
    all_same = map_lgl(first_letters, ~ length(unique(.x)) == 1)
  ) %>%
  filter(all_same)

nrow(title_alliteration)
```

## Part III: Pivoting (Hania)

* Create a new version of the `titles_basics` table that has one row for each title-genre combination. See the `separate_rows` function for a useful too here.
```{r new, echo = TRUE }
library(tidyr)
library(dplyr)

titles_genre_expanded <- title_basics %>%
   filter(!is.na(genres)) |>
  separate_rows(genres, sep = ",")
titles_genre_expanded
```

* Using that table, create a line plot of the count different genres over time (you may limit this to the most common genres if you wish).
```{r table, echo = TRUE}

genre_counts <- titles_genre_expanded %>%
  group_by(startYear, genres) %>%
  summarise(count = n()) %>%
  filter(!is.na(startYear))

top_genres <- genre_counts %>%
  group_by(genres) %>%
  summarise(total_count = sum(count)) %>%
  top_n(10, total_count) %>%
  pull(genres)

genre_counts_top <- genre_counts %>%
  filter(genres %in% top_genres)

ggplot(genre_counts_top, aes(x = startYear, y = count, color = genres)) +
  geom_line() +
  labs(
    title = "Count of Different Genres Over Time",
    x = "Year",
    y = "Genre Count"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

* Use the `model.matrix` function in the following way: `model.matrix(yourtalltable, ~ genre - 1)` to create a wide table with one column for each genre. Use this table to find the most common pair of genres (hint: use the `cor` function or produce facet plots)

```{r}
wide_genre_table <- model.matrix(~ genres - 1, data = titles_genre_expanded)
cor_matrix <- cor(wide_genre_table, use = "pairwise.complete.obs")
# Find the most common pair of genres
most_common_pair <- which(cor_matrix == max(cor_matrix[upper.tri(cor_matrix)]), arr.ind = TRUE)
most_common_genres <- colnames(cor_matrix)[most_common_pair]
cat("The most common pair of genres is:", 
    colnames(cor_matrix)[most_common_pair[1]], "and", 
    colnames(cor_matrix)[most_common_pair[2]], "\n")


```





## Part IV: Joining Tables

* Join the table with one title-genre per row from the previous section with the ratings table.

```{r}
genre_ratings <- title_basics %>%
  left_join(title_ratings, by = "tconst") %>%
  filter(!is.na(averageRating))

genre_ratings
```

  * What is the highest rated genre? What is the lowest rated genre?
  
```{r}
genre_ratings %>%
  group_by(genres) %>%
  summarise(mean_rating = mean(averageRating)) %>%
  arrange(desc(mean_rating)) %>%
  slice(1, n())
```
  
  * Using stacked bar charts, investigate the proportions of different genres over time. Are any incresing or decreasing? Use factor functions to help make the plots easier to read.
  
```{r}
genre_ratings %>%
  mutate(decade = floor(startYear / 10) * 10) %>%
  count(decade, genres) %>%
  group_by(decade) %>%
  mutate(prop = n / sum(n)) %>%
  filter(genres %in% c("Drama", "Comedy", "Action", "Horror")) %>%
  ggplot(aes(decade, prop, fill = fct_reorder(genres, prop))) +
  geom_col() +
  labs(title = "Genre Proportions") +
  scale_fill_brewer(palette = "Set2")
```

* Join the `title_basics` with the ratings table. Have the number of ratings changed over time (based on release year)? Display graphically but also answer with numerical results.

```{r}
title_basics %>%
  left_join(title_ratings, by = "tconst") %>%
  filter(startYear >= 1900) %>%
  group_by(startYear) %>%
  summarise(median_votes = median(numVotes, na.rm = TRUE)) %>%
  ggplot(aes(startYear, median_votes)) +
  geom_line() +
  geom_smooth(method = "lm", color = "red") +
  scale_y_log10() +
  labs(title = "Median Votes per Title Over Time")
```

```{r}
cor.test(title_basics$startYear, log(title_ratings$numVotes))
```

p value = 0.613 > 0.05, the number of ratings have not changed over time.

* Join the names with the ratings and the principals table. 


  * Group by individual people, find the top ten people based on the median rating of the titles they appear in.
  
```{r}
top_people <- title_principals %>%
  left_join(title_ratings, by = "tconst") %>%
  group_by(nconst) %>%
  summarise(median_rating = median(averageRating, na.rm = TRUE)) %>%
  arrange(desc(median_rating)) %>%
  slice(1:10) %>%
  left_join(name_basics, by = "nconst")
```
  
  * Find the proportions of genres for the the titles that include the top 10 rated principals.
  
```{r}
# Make sure title_basics is loaded
if (!exists("title_basics")) {
  title_basics <- read_rds("data/title_basics.rda")
}

# Create the title_genres table (long-form genre table) if it doesn't exist
if (!exists("title_genres")) {
  title_genres <- title_basics %>%
    filter(!is.na(genres)) %>%
    separate_rows(genres, sep = ",")
}

# Now perform the join and calculations
title_principals %>%
  inner_join(top_people, by = "nconst") %>%
  left_join(title_genres, by = "tconst") %>%
  count(genres) %>%
  mutate(prop = n / sum(n))
```
  
  * Graph ratings against years. What trends do you see?
  
```{r}
title_principals %>%
  left_join(title_basics, by = "tconst") %>%
  left_join(title_ratings, by = "tconst") %>%
  ggplot(aes(startYear, averageRating)) +
  geom_hex(bins = 30) +
  geom_smooth(method = "lm", color = "red")
```

* Create a table with one row for each person in the `name_basics` table and title they are known for. Join this to the ratings table to get the ratings of the "known for" films. Find the person (or people) who have the highest median known for rating.

```{r}
name_ratings <- name_basics %>%
  separate_rows(knownForTitles, sep = ",") %>%
  left_join(title_ratings, by = c("knownForTitles" = "tconst")) %>%
  group_by(nconst, primaryName) %>%
  summarise(median_rating = median(averageRating, na.rm = TRUE)) %>%
  arrange(desc(median_rating))

name_ratings %>% slice(1)
```



## Part V: Profiling and Parallel Processing

* These are large data sets (and yet only a sample of the entire IMDb!), so it make sense spend some time improving our code.
* Pick one or more of the previous problems and profile the performance of that piece. Write up your findings. If you see any opportunities to improve performance, feel fee to implement than and share the results.

```{r}
#–– Load packages once at the top
if (!requireNamespace("profvis", quietly = TRUE)) install.packages("profvis")
library(profvis)
library(dplyr)
library(tidyr)
library(ggplot2)

#–– Profile the genre-over-time pipeline
profvis({
  titles_genre_expanded <- title_basics %>%
    separate_rows(genres, sep = ",") %>%
    filter(!is.na(startYear), genres != "")

  genre_counts <- titles_genre_expanded %>%
    group_by(startYear, genres) %>%
    summarise(count = n(), .groups = "drop")

  ggplot(genre_counts, aes(x = startYear, y = count, color = genres)) +
    geom_line() +
    labs(
      title = "Title Counts by Genre Over Time",
      x     = "Year",
      y     = "Number of Titles"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
})


```
The profvis profile makes it clear that the vast majority of execution time—about 1.57 s out of 1.73 s, or nearly 90%—is spent inside separate_rows(), the step that unnests the comma‐delimited genre strings. All of the downstream operations (filtering, grouping, summarising, even plotting) together consume less than 10% of the total time.  

```{r}
#–– 1. Install & load packages
if (!requireNamespace("profvis", quietly = TRUE)) install.packages("profvis")
if (!requireNamespace("data.table", quietly = TRUE)) install.packages("data.table")
if (!requireNamespace("ggplot2",   quietly = TRUE)) install.packages("ggplot2")

library(profvis)
library(data.table)
library(ggplot2)

#–– 2. Profile the optimized pipeline
profvis({
  # Convert your titanic table to data.table
  dt <- as.data.table(title_basics)

  # Fast pivot: split & unnest genres per title
  titles_genre_dt <- dt[
    !is.na(startYear) & genres != "",
    .(   # this j-expression runs per group defined in 'by'
      primaryTitle,
      startYear,
      genres = unlist(strsplit(genres, split = ","))
    ),
    by = .(tconst)  # group by the unique title ID
  ]

  # Fast aggregation: count titles by year & genre
  genre_counts_dt <- titles_genre_dt[
    ,
    .(count = .N),
    by = .(startYear, genres)
  ]

  # Plot to include in the profile
  ggplot(genre_counts_dt, aes(x = startYear, y = count, color = genres)) +
    geom_line() +
    labs(
      title = "Title Counts by Genre Over Time (data.table)",
      x     = "Year",
      y     = "Number of Titles"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
})

```
After re‐profiling the fully optimized pipeline with profvis, total elapsed time has dropped from roughly 1.73 s to 0.79 s. The new flame graph shows that our “pivot to long” step—now implemented with strsplit(..., ",") plus unlist() inside a data.table[ , …, by=.(tconst)] call—accounts for about 560 ms (strsplit ~330 ms, unlist ~230 ms), while the subsequent data.table grouping costs only 70 ms, and the ggplot rendering is negligible. In contrast to the original tidyr::separate_rows() approach (which took ∼1.57 s), this change yields a 3× speed‐up on the unnesting phase and over a 2× improvement end‐to‐end—confirming that switching to a C-level split+unnest in data.table was an effective performance enhancement.


* Select a previous computation that could be improved using parallelization and implement a parallelization solution. Using `system.time` show that parallelization improves performance.
```{r final-chunk-with-parallel-and-genres, message=FALSE, warning=FALSE}
library(data.table)
library(parallel)
library(dplyr)
library(ggplot2)

#–– Prepare the long‐form genre table (equivalent to title_genres)
dt <- as.data.table(title_basics)
titles_genre_long <- dt[
  !is.na(startYear) & genres != "",
  .(
    tconst,
    primaryTitle,
    startYear,
    genres = unlist(strsplit(genres, split = ","))
  ),
  by = .(tconst)
]

#–– Now, calculate genre proportions using titles_genre_long
genre_proportions <- titles_genre_long %>%
  select(genres) %>% # Select only 'genres' to avoid duplicates
  count(genres, name = "genre_count") %>% # Use 'name' to control the count column name
  mutate(prop = genre_count / sum(genre_count))

print("Genre Proportions:")
print(genre_proportions)

#–– Parallel processing for genre counting (as before, but using titles_genre_long)
years <- unique(titles_genre_long$startYear)
ncores <- max(detectCores() - 1, 1)
cl     <- makeForkCluster(ncores)

t_serial_genre_count <- system.time({
  genre_counts_serial <- titles_genre_long[, .(count = .N), by = .(startYear, genres)]
})

t_parallel_genre_count <- system.time({
  year_lists <- parLapply(cl, years, function(yr) {
    sub <- titles_genre_long[startYear == yr]
    sub[, .(startYear = yr, genres, count = .N), by = genres]
  })
  genre_counts_parallel <- rbindlist(year_lists)
})

stopCluster(cl)

cat("\nSerial Genre Counting Time:\n")
print(t_serial_genre_count)
cat("\nParallel Genre Counting Time:\n")
print(t_parallel_genre_count)
```
There is a clear improvement when using parallel processing.


* One task we performed involved counting items in strings separated by commas. Propose two different functions that could perform this taks. Compare them using bench marking. Which version would you recommend?

```{r comma-count-benchmark, message=FALSE}
library(stringr)
library(microbenchmark)

# Sample 10,000 non-NA genre‐strings
set.seed(123)
test_vec <- sample(na.omit(title_basics$genres), 10000, replace = TRUE)

# Function 1: count commas via regex + add 1
count_commas <- function(x) {
  str_count(x, ",") + 1
}

# Function 2: split on commas & take lengths
count_split <- function(x) {
  lengths(strsplit(x, ","))
}

# Sanity check now passes
stopifnot(all(count_commas(test_vec) == count_split(test_vec)))

# Benchmark the two approaches
bm <- microbenchmark(
  regex_count = count_commas(test_vec),
  split_count = count_split(test_vec),
  times = 100L
)
print(bm)

```
Median times:

regex_count: 970.98 μs (~0.97 ms)

split_count: 4873.75 μs (~4.87 ms)

Speed‐up: The regex‐based approach is roughly 5× faster than the split-and-length method.

Recommendation: Use the concise, vectorized str_count(x, ",") + 1 version for counting comma-separated items.

## Part VI: Shiny Applications (Hania)


### Application 1

Using results from the previous section, create a shiny application that allows users to interact with the with the IMDb data. The application should use both interactive graphs and at least 3 widgets.


### Application 2

In the principals table, there is a `category` column. Use this column as a primary filter to allow users to then select specific job categories. After select the specific job categories, display information from another table.

## Extra Credit: 6 Degrees of Kevin Bacon

Create an app to allow users to play [Six Degrees of Kevin Bacon](https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon#:~:text=Six%20Degrees%20of%20Kevin%20Bacon%20or%20Bacon's%20Law%20is%20a,ultimately%20leads%20to%20prolific%20American).

Create a Shiny application where a person can type the primary title of movie or TV show. Then have app show all the people who had a role in the show. Let the user select a person in that cast and show all other people who have been in a title with that person. Repeat up to 6 times. If "Kevin Bacon" (`nconst == 'nm0000102'`) ever appears in the list, let the player know they have won! If they click more than 6 times, let them know they have lost.


